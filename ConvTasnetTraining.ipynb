{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5777687d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audioread in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (7.0.4)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from smart_open) (1.16.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\n",
      "Requirement already satisfied: sox in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sox) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sox) (4.9.0)\n",
      "Requirement already satisfied: librosa in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa) (21.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pooch>=1.0->librosa) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->lazy-loader>=0.1->librosa) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
      "Requirement already satisfied: torchaudio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchaudio) (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->torchaudio) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->torchaudio) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Important to run on a restarted kernel\n",
    "!pip install smart_open\n",
    "!pip install torchvision\n",
    "!pip install sox\n",
    "!pip install librosa\n",
    "!pip install torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from smart_open import open\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63bb0b",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b650c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples/data points per second.\n",
    "sample_rate=8000\n",
    "\n",
    "# the signal length of training sequences\n",
    "training_signal_len = 40000\n",
    "\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_length(sample, training_signal_len):\n",
    "    '''\n",
    "    Returns `sample` such that the length is equal to `training_signal_len`.\n",
    "    If the sample is too short, it will repeat the start of the input until enough samples\n",
    "    have been added.\n",
    "    '''\n",
    "    len_diff = training_signal_len - len(sample)\n",
    "    if len_diff < 0: # The sample is too long\n",
    "        return sample[:training_signal_len]\n",
    "    elif len_diff > 0: # The sample is too short\n",
    "        sample_dup = np.tile(sample, (len_diff // len(sample)) + 1)\n",
    "        return np.concatenate([sample, sample_dup[:len_diff]])\n",
    "    else:\n",
    "        return sample\n",
    "\n",
    "def dynamic_mix_data_prep(voice_dir, noise_dir, training_signal_len, sample_rate):\n",
    "    \"\"\"\n",
    "    This function defines the compute graph for dynamic mixing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly chooses files from voice and noise url lists\n",
    "    voice_dir_len = 2703\n",
    "    noise_dir_len = 2000\n",
    "    clean_file_idx, noise_file_idx = rng.choice(voice_dir_len, size=2, replace=False)\n",
    "    clean1_file, clean2_file = voice_dir[clean_file_idx], voice_dir[noise_file_idx]\n",
    "    noise_file = noise_dir[np.random.randint(noise_dir_len)]\n",
    "    \n",
    "    sources = []\n",
    "    first_lvl = None\n",
    "    spk_files = [clean2_file, noise_file, clean1_file]\n",
    "\n",
    "    for i, spk_file in enumerate(spk_files):\n",
    "        \n",
    "        # Load audio and ensure it's of minimum length\n",
    "        with open(spk_file, 'rb') as file:\n",
    "            # print(f\"File librosa.load({file})\")\n",
    "            y, sr = librosa.load(file, sr=sample_rate, duration=training_signal_len/sample_rate)\n",
    "            y = standardize_length(y, training_signal_len)\n",
    "\n",
    "        # Normalize audio\n",
    "        tmp = librosa.util.normalize(y)\n",
    "\n",
    "        # Layer on a stack\n",
    "        if i == 0 or i == 1:\n",
    "            gain = np.clip(random.normalvariate(-27.43, 2.57), -45, 0)\n",
    "            tmp *= 10 ** (gain / 20)  # Convert dB to amplitude\n",
    "            first_lvl = gain\n",
    "        else:\n",
    "            gain = np.clip(first_lvl + random.normalvariate(-2.51, 2.66), -45, 0)\n",
    "            tmp *= 10 ** (gain / 20)  # Convert dB to amplitude\n",
    "            \n",
    "        sources.append(tmp)\n",
    "\n",
    "    # Mix the sources together\n",
    "    mixture = sum(sources)\n",
    "\n",
    "    # Calculate the maximum amplitude among the tensors\n",
    "    max_amp = max(np.abs(mixture).max(), *[np.abs(s).max() for s in sources])\n",
    "\n",
    "    # Calculate scale value and apply it to the array\n",
    "    mix_scaling = 1 / max_amp * 0.9\n",
    "    mixture *= mix_scaling\n",
    "    \n",
    "    # Save the clean audio also\n",
    "    with open(clean1_file, 'rb') as file:\n",
    "        clean, _ = librosa.load(file, sr=sample_rate, duration=training_signal_len/sample_rate)\n",
    "        clean = standardize_length(clean, training_signal_len)\n",
    "        \n",
    "    clean *= 10 ** (gain / 20)  # Apply the same gain as mixture\n",
    "\n",
    "    return mixture, clean, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3dd62",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b92e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory_usage():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2  # Convert bytes to megabytes\n",
    "    r = torch.cuda.memory_reserved(0) / 1024 ** 2  # Convert bytes to megabytes\n",
    "    a = torch.cuda.memory_allocated(0) / 1024 ** 2  # Convert bytes to megabytes\n",
    "    max_allocated = torch.cuda.max_memory_allocated(0) / 1024 ** 2  # Convert bytes to megabytes\n",
    "    print(f\"Memory allocated: {a:.2f} MB\")\n",
    "    print(f\"Memory reserved: {r: .2f} MB\")\n",
    "    print(f\"Memory remaining: {t - r: .2f}\")\n",
    "    print(f\"Peak memory allocated: {max_allocated:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00fb544",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "def get_clean_and_noise_urls():\n",
    "    '''\n",
    "    Useful for interacting with S3 within a Sagemaker notebook instance. \n",
    "    This enables us to store only the urls for resources within S3 as \n",
    "    opposed to passing around all audio files.\n",
    "    \n",
    "    Returns (Clean URLs, Noise URLs)\n",
    "    '''\n",
    "\n",
    "    # Initialize the S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Specify the S3 bucket name\n",
    "    bucket_name = 'amdmic-librispeech'\n",
    "\n",
    "    # Define the paths in S3\n",
    "    clean_speech_s3_path = \"clean_speech/clean_speech/\"\n",
    "    noise_s3_path = \"noise/\"\n",
    "\n",
    "    # Store urls of our dataset\n",
    "    clean = np.array([])\n",
    "    noise = np.array([])\n",
    "\n",
    "    # Load clean speech urls\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=clean_speech_s3_path):\n",
    "        for content in page.get('Contents', []):\n",
    "            object_key = content['Key']\n",
    "            url = f\"s3://{bucket_name}/{object_key}\"\n",
    "            clean = np.append(clean, url)\n",
    "\n",
    "    print(f'Number of clean audio files are {len(clean)}')\n",
    "\n",
    "    # Load noise urls\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=noise_s3_path):\n",
    "        for content in page.get('Contents', []):\n",
    "            object_key = content['Key']\n",
    "            url = f\"s3://{bucket_name}/{object_key}\"\n",
    "            noise = np.append(noise, url)\n",
    "\n",
    "    print(f'Number of Noisy audio files are {len(noise)}')\n",
    "    return clean, noise\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, transform=None, target_transform=None, num_samples=100):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        clean_dir, noise_dir = get_clean_and_noise_urls()\n",
    "        self.voice_dir = clean_dir\n",
    "        self.noise_dir = noise_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mixture, clean, _ = dynamic_mix_data_prep(self.voice_dir, self.noise_dir, training_signal_len, sample_rate)\n",
    "                \n",
    "        return torch.tensor(standardize_length(mixture, training_signal_len)), torch.tensor(standardize_length(clean, training_signal_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165a3fe",
   "metadata": {},
   "source": [
    "# Conv-TasNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0bb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.475121 Mb\n",
      "torch.Size([320])\n"
     ]
    }
   ],
   "source": [
    "class GlobalLayerNorm(nn.Module):\n",
    "    '''\n",
    "       Calculate Global Layer Normalization\n",
    "       dim: (int or list or torch.Size) –\n",
    "            input shape from an expected input of size\n",
    "       eps: a value added to the denominator for numerical stability.\n",
    "       elementwise_affine: a boolean value that when set to True,\n",
    "           this module has learnable per-element affine parameters\n",
    "           initialized to ones (for weights) and zeros (for biases).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, eps=1e-05, elementwise_affine=True):\n",
    "        super(GlobalLayerNorm, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(self.dim, 1))\n",
    "            self.bias = nn.Parameter(torch.zeros(self.dim, 1))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = N x C x L\n",
    "        # N x 1 x 1\n",
    "        # cln: mean,var N x 1 x L\n",
    "        # gln: mean,var N x 1 x 1\n",
    "        if x.dim() != 3:\n",
    "            raise RuntimeError(\"{} accept 3D tensor as input\".format(\n",
    "                self.__class__.__name__))\n",
    "\n",
    "        mean = torch.mean(x, (1, 2), keepdim=True)\n",
    "        var = torch.mean((x-mean)**2, (1, 2), keepdim=True)\n",
    "        # N x C x L\n",
    "        if self.elementwise_affine:\n",
    "            x = self.weight*(x-mean)/torch.sqrt(var+self.eps)+self.bias\n",
    "        else:\n",
    "            x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CumulativeLayerNorm(nn.LayerNorm):\n",
    "    '''\n",
    "       Calculate Cumulative Layer Normalization\n",
    "       dim: you want to norm dim\n",
    "       elementwise_affine: learnable per-element affine parameters\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, elementwise_affine=True):\n",
    "        super(CumulativeLayerNorm, self).__init__(\n",
    "            dim, elementwise_affine=elementwise_affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: N x C x L\n",
    "        # N x L x C\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        # N x L x C == only channel norm\n",
    "        x = super().forward(x)\n",
    "        # N x C x L\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def select_norm(norm, dim):\n",
    "    if norm not in ['gln', 'cln', 'bn']:\n",
    "        raise ValueError(\"Unsupported norm type\")\n",
    "    if norm == 'gln':\n",
    "        return GlobalLayerNorm(dim, elementwise_affine=True)\n",
    "    if norm == 'cln':\n",
    "        return CumulativeLayerNorm(dim, elementwise_affine=True)\n",
    "    else:\n",
    "        return nn.BatchNorm1d(dim)\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    '''\n",
    "       Applies a 1D convolution over an input signal composed of several input planes.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv1D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x, squeeze=False):\n",
    "        # x: N x C x L\n",
    "        if x.dim() not in [2, 3]:\n",
    "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
    "                self.__class__.__name__))\n",
    "        x = super().forward(x if x.dim() == 3 else torch.unsqueeze(x, 1))\n",
    "        if squeeze:\n",
    "            x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTrans1D(nn.ConvTranspose1d):\n",
    "    '''\n",
    "       This module can be seen as the gradient of Conv1d with respect to its input.\n",
    "       It is also known as a fractionally-strided convolution\n",
    "       or a deconvolution (although it is not an actual deconvolution operation).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ConvTrans1D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x, squeeze=False):\n",
    "        \"\"\"\n",
    "        x: N x L or N x C x L\n",
    "        \"\"\"\n",
    "        if x.dim() not in [2, 3]:\n",
    "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
    "                self.__class__.__name__))\n",
    "        x = super().forward(x if x.dim() == 3 else torch.unsqueeze(x, 1))\n",
    "        if squeeze:\n",
    "            x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "class Conv1D_Block(nn.Module):\n",
    "    '''\n",
    "       Consider only residual links\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels=256, out_channels=512,\n",
    "                 kernel_size=3, dilation=1, norm='gln', causal=False):\n",
    "        super(Conv1D_Block, self).__init__()\n",
    "        # conv 1 x 1\n",
    "        self.conv1x1 = Conv1D(in_channels, out_channels, 1)\n",
    "        self.PReLU_1 = nn.PReLU()\n",
    "        self.norm_1 = select_norm(norm, out_channels)\n",
    "        # not causal don't need to padding, causal need to pad+1 = kernel_size\n",
    "        self.pad = (dilation * (kernel_size - 1)) // 2 if not causal else (\n",
    "            dilation * (kernel_size - 1))\n",
    "        # depthwise convolution\n",
    "        self.dwconv = Conv1D(out_channels, out_channels, kernel_size,\n",
    "                             groups=out_channels, padding=self.pad, dilation=dilation)\n",
    "        self.PReLU_2 = nn.PReLU()\n",
    "        self.norm_2 = select_norm(norm, out_channels)\n",
    "        self.Sc_conv = nn.Conv1d(out_channels, in_channels, 1, bias=True)\n",
    "        self.causal = causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: N x C x L\n",
    "        # N x O_C x L\n",
    "        c = self.conv1x1(x)\n",
    "        # N x O_C x L\n",
    "        c = self.PReLU_1(c)\n",
    "        c = self.norm_1(c)\n",
    "        # causal: N x O_C x (L+pad)\n",
    "        # noncausal: N x O_C x L\n",
    "        c = self.dwconv(c)\n",
    "        # N x O_C x L\n",
    "        if self.causal:\n",
    "            c = c[:, :, :-self.pad]\n",
    "        c = self.Sc_conv(c)\n",
    "        return x+c\n",
    "\n",
    "\n",
    "class ConvTasNet(nn.Module):\n",
    "    '''\n",
    "       ConvTasNet module\n",
    "       N\tNumber of ﬁlters in autoencoder\n",
    "       L\tLength of the ﬁlters (in samples)\n",
    "       B\tNumber of channels in bottleneck and the residual paths’ 1 × 1-conv blocks\n",
    "       Sc\tNumber of channels in skip-connection paths’ 1 × 1-conv blocks\n",
    "       H\tNumber of channels in convolutional blocks\n",
    "       P\tKernel size in convolutional blocks\n",
    "       X\tNumber of convolutional blocks in each repeat\n",
    "       R\tNumber of repeats\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 N=512,\n",
    "                 L=16,\n",
    "                 B=128,\n",
    "                 H=512,\n",
    "                 P=3,\n",
    "                 X=8,\n",
    "                 R=3,\n",
    "                 norm=\"gln\",\n",
    "                 num_spks=2,\n",
    "                 activate=\"relu\",\n",
    "                 causal=False):\n",
    "        super(ConvTasNet, self).__init__()\n",
    "        # n x 1 x T => n x N x T\n",
    "        self.encoder = Conv1D(1, N, L, stride=L // 2, padding=0)\n",
    "        # n x N x T  Layer Normalization of Separation\n",
    "        self.LayerN_S = select_norm('cln', N)\n",
    "        # n x B x T  Conv 1 x 1 of  Separation\n",
    "        self.BottleN_S = Conv1D(N, B, 1)\n",
    "        # Separation block\n",
    "        # n x B x T => n x B x T\n",
    "        self.separation = self._Sequential_repeat(\n",
    "            R, X, in_channels=B, out_channels=H, kernel_size=P, norm=norm, causal=causal)\n",
    "        # n x B x T => n x 2*N x T\n",
    "        self.gen_masks = Conv1D(B, num_spks*N, 1)\n",
    "        # n x N x T => n x 1 x L\n",
    "        self.decoder = ConvTrans1D(N, 1, L, stride=L//2)\n",
    "        # activation function\n",
    "        active_f = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'softmax': nn.Softmax(dim=0)\n",
    "        }\n",
    "        self.activation_type = activate\n",
    "        self.activation = active_f[activate]\n",
    "        self.num_spks = num_spks\n",
    "\n",
    "    def _Sequential_block(self, num_blocks, **block_kwargs):\n",
    "        '''\n",
    "           Sequential 1-D Conv Block\n",
    "           input:\n",
    "                 num_block: how many blocks in every repeats\n",
    "                 **block_kwargs: parameters of Conv1D_Block\n",
    "        '''\n",
    "        Conv1D_Block_lists = [Conv1D_Block(\n",
    "            **block_kwargs, dilation=(2**i)) for i in range(num_blocks)]\n",
    "\n",
    "        return nn.Sequential(*Conv1D_Block_lists)\n",
    "\n",
    "    def _Sequential_repeat(self, num_repeats, num_blocks, **block_kwargs):\n",
    "        '''\n",
    "           Sequential repeats\n",
    "           input:\n",
    "                 num_repeats: Number of repeats\n",
    "                 num_blocks: Number of block in every repeats\n",
    "                 **block_kwargs: parameters of Conv1D_Block\n",
    "        '''\n",
    "        repeats_lists = [self._Sequential_block(\n",
    "            num_blocks, **block_kwargs) for i in range(num_repeats)]\n",
    "        return nn.Sequential(*repeats_lists)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() >= 3:\n",
    "            raise RuntimeError(\n",
    "                \"{} accept 1/2D tensor as input, but got {:d}\".format(\n",
    "                    self.__class__.__name__, x.dim()))\n",
    "        if x.dim() == 1:\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "        # x: n x 1 x L => n x N x T\n",
    "        w = self.encoder(x)\n",
    "        # n x N x L => n x B x L\n",
    "        e = self.LayerN_S(w)\n",
    "        e = self.BottleN_S(e)\n",
    "        # n x B x L => n x B x L\n",
    "        e = self.separation(e)\n",
    "        # n x B x L => n x num_spk*N x L\n",
    "        m = self.gen_masks(e)\n",
    "        # n x N x L x num_spks\n",
    "        m = torch.chunk(m, chunks=self.num_spks, dim=1)\n",
    "        # num_spks x n x N x L\n",
    "        m = self.activation(torch.stack(m, dim=0))\n",
    "        d = [w*m[i] for i in range(self.num_spks)]\n",
    "        # decoder part num_spks x n x L\n",
    "        s = [self.decoder(d[i], squeeze=True) for i in range(self.num_spks)]\n",
    "        return s\n",
    "\n",
    "\n",
    "def check_parameters(net):\n",
    "    '''\n",
    "        Returns module parameters. Mb\n",
    "    '''\n",
    "    parameters = sum(param.numel() for param in net.parameters())\n",
    "    return parameters / 10**6\n",
    "\n",
    "\n",
    "def test_convtasnet():\n",
    "    x = torch.randn(320)\n",
    "    nnet = ConvTasNet()\n",
    "    s = nnet(x)\n",
    "    print(str(check_parameters(nnet))+' Mb')\n",
    "    print(s[1].shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_convtasnet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7912a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae764192",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23369a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def sisnr(x, s, eps=1e-8):\n",
    "    \"\"\"\n",
    "    calculate training loss\n",
    "    input:\n",
    "          x: separated signal, N x S tensor\n",
    "          s: reference signal, N x S tensor\n",
    "    Return:\n",
    "          sisnr: N tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def l2norm(mat, keepdim=False):\n",
    "        return torch.norm(mat, dim=-1, keepdim=keepdim)\n",
    "\n",
    "    if x.shape != s.shape:\n",
    "        raise RuntimeError(\n",
    "            \"Dimention mismatch when calculate si-snr, {} vs {}\".format(\n",
    "                x.shape, s.shape))\n",
    "    x_zm = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "    s_zm = s - torch.mean(s, dim=-1, keepdim=True)\n",
    "    t = torch.sum(\n",
    "        x_zm * s_zm, dim=-1,\n",
    "        keepdim=True) * s_zm / (l2norm(s_zm, keepdim=True)**2 + eps)\n",
    "    return 20 * torch.log10(eps + l2norm(t) / (l2norm(x_zm - t) + eps))\n",
    "\n",
    "\n",
    "def si_snr_loss(ests, egs):\n",
    "    # spks x n x S\n",
    "    print(egs.shape, ests.shape)\n",
    "    print(egs)\n",
    "    refs = egs\n",
    "#     num_spks = len(refs)\n",
    "    num_spks = refs.size(0)\n",
    "\n",
    "    def sisnr_loss(permute):\n",
    "        # for one permute\n",
    "        return sum(\n",
    "            [sisnr(ests[s], refs[t])\n",
    "             for s, t in enumerate(permute)]) / len(permute)\n",
    "        # average the value\n",
    "\n",
    "    # P x N\n",
    "#     N = egs[\"mix\"].size(0)\n",
    "    N = egs.size(1)\n",
    "    sisnr_mat = torch.stack(\n",
    "        [sisnr_loss(p) for p in permutations(range(num_spks))])\n",
    "    max_perutt, _ = torch.max(sisnr_mat, dim=0)\n",
    "    # si-snr\n",
    "    return -torch.sum(max_perutt) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04824e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save periodic checkpoints.\n",
    "!mkdir checkpoints\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = torch.sum(signal ** 2) / signal.numel()\n",
    "    noise_power = torch.sum(noise ** 2) / noise.numel()\n",
    "\n",
    "    linear_snr = signal_power / noise_power\n",
    "    return linear_snr\n",
    "\n",
    "# TODO:\n",
    "# 1. Do we need mixed precision / torch.autocast?\n",
    "# Speechbrain uses it on CPU fp16, or CUDA fp16/bf16:\n",
    "# https://github.com/speechbrain/speechbrain/blob/5a8535c7bb202ecc852223f4a200b1f1bbc248fb/speechbrain/core.py#L790C1-L794C32\n",
    "# 2. (Done? Unclear why the threshold is -30) Consider loss thresholding:\n",
    "# https://github.com/speechbrain/speechbrain/blob/1350e9b3cebae9f78e57e97d82a2d89ba3fc2ae1/recipes/WSJ0Mix/separation/train.py#L156C1-L160C43\n",
    "# 3. (Done?) Consider gradient clipping, skipping batches with bad losses:\n",
    "# https://github.com/speechbrain/speechbrain/blob/1350e9b3cebae9f78e57e97d82a2d89ba3fc2ae1/recipes/WSJ0Mix/separation/train.py#L164C1-L181C64\n",
    "\n",
    "# Reference hyperparameters:\n",
    "# https://github.com/speechbrain/speechbrain/blob/1350e9b3cebae9f78e57e97d82a2d89ba3fc2ae1/recipes/WSJ0Mix/separation/hparams/sepformer.yaml\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader,\n",
    "                num_epochs=50, threshold=-30, loss_upper_lim=999999, clip_grad_norm=5):\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    snrs = []\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=\" * 50)\n",
    "        count = 0\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        iter = 0\n",
    "        nonfinite_count = 0\n",
    "        for inputs, targets in train_loader:            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            # predicted_mask = model(inputs)\n",
    "            outputs = model(inputs)\n",
    "#             outputs = torch.stack(outputs).clone().detach().requires_grad_(True)[0]\n",
    "\n",
    "#             outputs = outputs[0]\n",
    "\n",
    "            # Apply the predicted mask to the input data (element-wise multiplication)\n",
    "            # masked_output = predicted_mask * inputs\n",
    "            # This is done by forward pass (for Separation at least), so we're not doing this anymore?\n",
    "\n",
    "            # Calculate loss - compare masked output with clean audio (targets)\n",
    "            loss = criterion(outputs[0], targets)\n",
    "            print(loss)\n",
    "\n",
    "            # Loss thresholding\n",
    "            loss = loss[loss > threshold]\n",
    "            if loss.nelement() > 0:\n",
    "                loss = loss.mean()  # Does this make sense?\n",
    "\n",
    "            # Backward pass, gradient clipping, optimization\n",
    "            if loss.nelement() > 0 and loss < loss_upper_lim:\n",
    "                loss.backward()\n",
    "                if clip_grad_norm >= 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(),\n",
    "                        clip_grad_norm,\n",
    "                    )\n",
    "\n",
    "                optimizer.step()\n",
    "                iter += 1\n",
    "            else:\n",
    "                nonfinite_count += 1\n",
    "                print(\"infinite loss or empty loss! it happened {} times so far - skipping this batch\".format(\n",
    "                            nonfinite_count\n",
    "                     )\n",
    "                )\n",
    "                loss.data = torch.tensor(0.0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            count += 1\n",
    "            \n",
    "            # TO HELP DEBUG MEMORY ISSUES\n",
    "#             log_memory_usage()\n",
    "            \n",
    "            if iter % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Iter [{iter}/{len(train_loader)}], Loss: {train_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        snr_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs[0], targets)\n",
    "                print(loss)\n",
    "                if loss.nelement() > 0:\n",
    "                    loss = loss.mean()  # Does this make sense?\n",
    "                val_loss += loss.item()\n",
    "                snr_ratio = calculate_snr(outputs[0], targets)\n",
    "                snr_sum += snr_ratio.item()\n",
    "\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        snr_sum /= len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        snrs.append(snr_sum)\n",
    "        \n",
    "        # TO HELP DEBUG MEMORY ISSUES\n",
    "#         log_memory_usage()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val SNR: {10 * np.log10(snr_sum):.4f}, LR: {optimizer.param_groups[-1]['lr']}\")\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'training_loss': train_loss, 'val_loss': val_loss},f'checkpoints/detector_epoch_{epoch + 1}.pth')\n",
    "\n",
    "\n",
    "    # Save the model after training\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0597bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Train Dataset:\n",
      "Number of clean audio files are 2703\n",
      "Number of Noisy audio files are 2000\n",
      "time is -1.6264114379882812\n",
      "Initializing Validation Dataset:\n",
      "Number of clean audio files are 2703\n",
      "Number of Noisy audio files are 2000\n",
      "time is -1.2173388004302979\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation sets\n",
    "train_samples = 10_000\n",
    "val_samples = 2_000\n",
    "\n",
    "print(\"Initializing Train Dataset:\")\n",
    "train_dataset = CustomDataset(num_samples=train_samples)\n",
    "print(\"Initializing Validation Dataset:\")\n",
    "val_dataset = CustomDataset(num_samples=val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Conv-Tas-Net model\n",
    "model = ConvTasNet()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = sisnr  # alternatively, criterion = cal_si_snr\n",
    "# criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a607e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
